{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "#import seaborn as sns\n",
    "#from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.calibration import calibration_curve,CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### need a separate clean_and_featurize for Lasso and BERT, respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_featurize(df,text_name='Report_Text_clean',BERT=True):\n",
    "    \"\"\"Something is happening here to clean the data....\"\"\"\n",
    "    labeled = df\n",
    "    #labeled = labeled[['Report_ID','Report_Text','MLS_mm']]\n",
    "    \n",
    "    #rename columns to relevant\n",
    "    labeled = labeled.rename(columns={\"Report_ID\": \"Report_Number\"})\n",
    "    \n",
    "    # Keep only those with IMPRESSIONS\n",
    "    #labeled = labeled.iloc[[x for x in range(labeled.shape[0]) if 'IMPRESSION:' in labeled.Report_Text.iloc[x]]]\n",
    "    \n",
    "    # replace whitespace with space ***************************\n",
    "    labeled[text_name] = labeled['Report_Text'].apply(lambda text: ' '.join(text.split()))\n",
    "    \n",
    "    #REMOVE HEADER:\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: re.split('-'*78, text, 1)[-1])\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: re.split('HISTORY:', text, 1)[-1])\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: re.split('REPORT ', text, 1)[-1])\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: re.split('REPORT:', text, 1)[-1])\n",
    "    \n",
    "    #REMOVE FOOTER:\n",
    "    # Remove footer parts\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: \n",
    "                                                       re.split('electronically signed by:', text, flags=re.IGNORECASE)[0])\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: \n",
    "                                                       ''.join(re.split('i, the teaching physician, have reviewed the images and agree with the report as written', text, flags=re.IGNORECASE)))\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: \n",
    "                                                       re.split('radiologists: signatures:', text, flags=re.IGNORECASE)[0])\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: \n",
    "                                                       re.split('providers: signatures:', text, flags=re.IGNORECASE)[0])\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: \n",
    "                                                       re.split('findings were discussed on', text, flags=re.IGNORECASE)[0])\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: \n",
    "                                                       re.split('this report was electronically signed by', text, flags=re.IGNORECASE)[0])\n",
    "\n",
    "    # Remove reference texts =====\n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: ''.join([x for i,x in enumerate(text.split('='*34)) if i != 1]))\n",
    "    \n",
    "    labeled[text_name] = labeled[text_name].apply(lambda text: text.lower())\n",
    "    \n",
    "    \n",
    "    if not BERT: #if it's not BERT then go ahead and replace things with n-grams\n",
    "    # Replace ngrams in Report_Text & IMPRESSION with their units\n",
    "        for group in wordgroups:\n",
    "            labeled[text_name] = labeled[text_name].apply(lambda text: text.replace(group, ''.join(group.split())))\n",
    "\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    #labeled = labeled.drop_duplicates(subset=['Report_Number'])\n",
    "    labeled = labeled.reset_index(drop=True)\n",
    "    \n",
    "    return labeled\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_BOW_matrix_2(df1,df2,vectorizer='tfidf',doc_col_name='Report_Text_clean'):\n",
    "    \"\"\"Takes a DataFrame in which there is a column of documents, then parses through those\n",
    "    texts in order to create a matrix in which the entries are the token counts per document.\n",
    "    Returns this matrix as a DataFrame\"\"\"\n",
    "    #initialize vectorizer\n",
    "    if vectorizer=='count':\n",
    "        vectorizer = CountVectorizer(min_df=2) #ignore if term appears in <cutoff % of documents\n",
    "    else:\n",
    "        vectorizer=TfidfVectorizer(min_df=2)\n",
    "    #Obtain counts using the column name corresponding to the radiology reports\n",
    "    vectorizer.fit(df1[doc_col_name])\n",
    "    \n",
    "    \n",
    "    train_vocab=vectorizer.transform(df1[doc_col_name]).todense()\n",
    "    test_vocab=vectorizer.transform(df2[doc_col_name]).todense()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    \n",
    "    #Each one of the elements of the table \n",
    "    X_train = pd.DataFrame(train_vocab, columns = feature_names)\n",
    "    X_test=pd.DataFrame(test_vocab,columns=feature_names)\n",
    "    \n",
    "    return X_train,X_test\n",
    "\n",
    "def generate_mcnemar_table(df):\n",
    "    \"\"\"assume that the final column is the ground_truth and that you have two classifiers that are being\n",
    "    compared\"\"\"\n",
    "    ground_truth=df['true_label']\n",
    "    \n",
    "    lasso_corrects=[]\n",
    "    bert_corrects=[]\n",
    "    \n",
    "    for _,row in df.iterrows():\n",
    "        if row['lasso_pred']==row['true_label']:\n",
    "            lasso_corrects.append(1)\n",
    "        else:\n",
    "            lasso_corrects.append(0)\n",
    "            \n",
    "\n",
    "    for _,row in df.iterrows():\n",
    "        if row['bert_pred']==row['true_label']:\n",
    "            bert_corrects.append(1)\n",
    "        else:\n",
    "            bert_corrects.append(0)    \n",
    "    \n",
    "    \n",
    "    df2=pd.DataFrame(columns=['lasso_correct','bert_correct'])\n",
    "    df2['lasso_correct']=lasso_corrects\n",
    "    df2['bert_correct']=bert_corrects\n",
    "    \n",
    "    a=0\n",
    "    b=0\n",
    "    c=0\n",
    "    d=0\n",
    "    \n",
    "    for _,row in df2.iterrows():\n",
    "        if np.logical_and(row['lasso_correct']==1,row['bert_correct']==1):\n",
    "            a+=1\n",
    "        elif np.logical_and(row['lasso_correct']==1,row['bert_correct']==0):\n",
    "            b+=1\n",
    "        elif np.logical_and(row['lasso_correct']==0,row['bert_correct']==1):\n",
    "            c+=1\n",
    "        elif np.logical_and(row['lasso_correct']==0,row['bert_correct']==0):\n",
    "            d+=1\n",
    "    \n",
    "    table=np.array([[a,b],[c,d]])\n",
    "    \n",
    "    return table\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dry Run Script to Construct McNemar Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('6.1.21 deidComplete REDCap Reports Non-duplicate Version 3.csv')\n",
    "df.rename(columns={'report_text': 'Report_Text', 'mls_mm_v2': 'MLS_mm','record_id':'Report_ID','ecass_v2':'ECASS','edema_severity_report':'edema_severity'}, inplace=True)\n",
    "wordgroups = list(pd.read_excel('wordgroups.xlsx',engine='openpyxl')['Word Groupings'])#now clean and featurize\n",
    "\n",
    "\n",
    "df=df.dropna(subset=['ivh_v2'])\n",
    "df['ivh_present']=df['ivh_v2'].apply(lambda x: 0 if x==1.0 else 1)\n",
    "df['ECASS']=df['ECASS'].fillna(0)\n",
    "df['severe_ecass']=df['ECASS'].apply(lambda x: 0 if x<3.0 else 1)\n",
    "df['MLS_mm']=df['MLS_mm'].fillna(0)\n",
    "df['MLS_presence']=df['MLS_mm'].apply(lambda x: 0 if x==0 else 1)\n",
    "#df_edema = df[np.logical_or(df['MLS_presence']==0,df['MLS_presence']==1)]\n",
    "df['edema_report']=df['edema_report'].fillna(0)\n",
    "df['edema_report']=df['edema_report'].apply(lambda x: int(x))\n",
    "\n",
    "input_spreadsheet=df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Training a LogReg and BERT side-by-side on same data split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for trial  1\n",
      "performed first split of the overall data\n",
      "performed featurization and split for Lasso\n",
      "423\n",
      "423\n",
      "(423, 2)\n",
      "Index(['text', 'labels'], dtype='object')\n",
      "[0.0021929824561403508, 0.0008110300081103001]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_trials=1\n",
    "\n",
    "outcome='edema_report'\n",
    "\n",
    "for trial in range(1,num_trials+1):\n",
    "    print('Results for trial ',trial)\n",
    "    \n",
    "    input_spreadsheet=input_spreadsheet[np.logical_or(input_spreadsheet[outcome]==0,input_spreadsheet[outcome]==1)]\n",
    "    \n",
    "    \n",
    "    ####MULTIPLE LEVELS OF TRAIN TEST SPLIT GOING ON HERE:\n",
    "    #1: Overall train and test: \n",
    "    #reminder: y_overall_train and y_overall_test will be shared by both models\n",
    "    X_overall_train,X_overall_test=train_test_split(input_spreadsheet,test_size=0.20)\n",
    "    y_overall_train,y_overall_test=X_overall_train[outcome],X_overall_test[outcome]\n",
    "    \n",
    "    print('performed first split of the overall data')\n",
    "    \n",
    "\n",
    "    \n",
    "    #2.derive BOW matrix for Lasso:\n",
    "    #below we create BOW matrix from the n-gram converted reports\n",
    "    X_lasso_train,X_lasso_test=create_BOW_matrix_2(clean_and_featurize(X_overall_train,BERT=False),\n",
    "                                                  clean_and_featurize(X_overall_test,BERT=False)) \n",
    "\n",
    "    \n",
    "#     print(X_lasso_train.shape)\n",
    "#     print(X_lasso_test.shape)\n",
    "#     #print(y_res.shape)\n",
    "    \n",
    "    print('performed featurization and split for Lasso')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    tuned_parameters =[{'tol': np.arange(0.0001, 0.1 + 0.01, 0.01)}]\n",
    "\n",
    "        #initialize Logistic Regression with GridSearch over 'tolerance' parameter\n",
    "    outcome_presence = GridSearchCV(LogisticRegression(penalty = 'l1',max_iter=1000, random_state=100, solver='liblinear'), \n",
    "                                tuned_parameters, cv = 10, scoring = 'roc_auc')\n",
    "\n",
    "                #Calibrate the classifier with Platt scaling\n",
    "    outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "    #SMOTE resampling\n",
    "    outcome_presence.fit(X_lasso_train,y_overall_train)\n",
    "    #derive the y predictions from lasso\n",
    "    y_lasso_pred = outcome_presence.predict(X_lasso_test) #THIS PART IS ALL FINISHED NOW!\n",
    "    \n",
    "    print(len(y_lasso_pred))\n",
    "    print(len(y_overall_test))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #3. derive proper training inputs for BERT from X_overall_train\n",
    "    #for simpletransformers library, must have 'text' and 'labels' columns\n",
    "    renamed_df=X_overall_train.rename(columns={outcome:'labels'}) # <--------------------------------COME BACK TO THIS PART TOMORROW\n",
    "    X_bert_train=clean_and_featurize(renamed_df,text_name='text')\n",
    "    X_bert_train=X_bert_train[['text','labels']]\n",
    "    \n",
    "    \n",
    "    #now derive BERT testing dataframe\n",
    "    renamed_test_df=X_overall_test.rename(columns={outcome:'labels'})\n",
    "    X_bert_test=clean_and_featurize(renamed_test_df,text_name='text')\n",
    "    X_bert_test=X_bert_test[['text','labels']]\n",
    "    \n",
    "    print(X_bert_test.shape)\n",
    "    print(X_bert_test.columns)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #derive weights for training:\n",
    "    num_zeros=X_bert_train[X_bert_train['labels']==0].shape[0]\n",
    "    num_ones=X_bert_train[X_bert_train['labels']==1].shape[0]\n",
    "    #derived weights\n",
    "    weights=[1/num_zeros,1/num_ones]\n",
    "    \n",
    "    print(weights)\n",
    "    \n",
    "#     #initialize your model\n",
    "#     model=ClassificationModel('bert','emilyalsentzer/Bio_ClinicalBERT',\n",
    "#                          num_labels=2,use_cuda=True,weight=weights,\n",
    "#                          args={'num_train_epochs':5,'output_dir':'mcnemar_outputs/','max_seq_length':512})\n",
    "    \n",
    "#     model.train_model(X_bert_train,num_training_epochs=10)\n",
    "    \n",
    "#     _,y_bert_pred,_=model.eval_model(X_bert_test) #<----NEED A BERT TESTING \n",
    "    \n",
    "#     print(X_bert_train.shape)\n",
    "#     print(y_overall_train.shape)\n",
    "#     print(X_bert_train.columns)\n",
    "\n",
    "#     #Now fill in the dataframe from which to derive McNemar cells\n",
    "#     df=pd.DataFrame(columns=['lasso_pred','bert_pred','true_label'])\n",
    "#     df['true_label']=y_overall_test\n",
    "#     df['lasso_pred']=y_lasso_pred\n",
    "#     df['bert_pred']=y_bert_pred\n",
    "    \n",
    "    \n",
    "#     #now generate McNemar table and print statistic:\n",
    "#     mcnemar_table=generate_mcnemar_table(df)\n",
    "#     print(mcnemar(mcnemar_table))\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_mcnemar_calc(input_spreadsheet,outcome,num_trials=5):\n",
    "    \n",
    "    #only calculate on severe_ecass\n",
    "    if outcome=='severe_ecass':\n",
    "        input_spreadsheet=input_spreadsheet[input_spreadsheet['hem_conv_v2']==1]\n",
    "    else:\n",
    "        input_spreadsheet=input_spreadsheet\n",
    "\n",
    "\n",
    "\n",
    "    for trial in range(1,num_trials+1):\n",
    "        print('Results for trial ',trial)\n",
    "\n",
    "        input_spreadsheet=input_spreadsheet[np.logical_or(input_spreadsheet[outcome]==0,input_spreadsheet[outcome]==1)]\n",
    "\n",
    "\n",
    "        ####MULTIPLE LEVELS OF TRAIN TEST SPLIT GOING ON HERE:\n",
    "        #1: Overall train and test: \n",
    "        #reminder: y_overall_train and y_overall_test will be shared by both models\n",
    "        X_overall_train,X_overall_test=train_test_split(input_spreadsheet,test_size=0.20)\n",
    "        y_overall_train,y_overall_test=X_overall_train[outcome],X_overall_test[outcome]\n",
    "\n",
    "        print('performed first split of the overall data')\n",
    "\n",
    "\n",
    "\n",
    "        #2.derive BOW matrix for Lasso:\n",
    "        #below we create BOW matrix from the n-gram converted reports\n",
    "        X_lasso_train,X_lasso_test=create_BOW_matrix_2(clean_and_featurize(X_overall_train,BERT=False),\n",
    "                                                      clean_and_featurize(X_overall_test,BERT=False)) \n",
    "\n",
    "\n",
    "    #     print(X_lasso_train.shape)\n",
    "    #     print(X_lasso_test.shape)\n",
    "    #     #print(y_res.shape)\n",
    "\n",
    "        print('performed featurization and split for Lasso')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        tuned_parameters =[{'tol': np.arange(0.0001, 0.1 + 0.01, 0.01)}]\n",
    "\n",
    "            #initialize Logistic Regression with GridSearch over 'tolerance' parameter\n",
    "        outcome_presence = GridSearchCV(LogisticRegression(penalty = 'l1',max_iter=1000, random_state=100, solver='liblinear'), \n",
    "                                    tuned_parameters, cv = 10, scoring = 'roc_auc')\n",
    "\n",
    "                    #Calibrate the classifier with Platt scaling\n",
    "        outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "        #SMOTE resampling\n",
    "        outcome_presence.fit(X_lasso_train,y_overall_train)\n",
    "        #derive the y predictions from lasso\n",
    "        y_lasso_pred = outcome_presence.predict(X_lasso_test) #THIS PART IS ALL FINISHED NOW!\n",
    "\n",
    "        print(len(y_lasso_pred))\n",
    "        print(len(y_overall_test))\n",
    "\n",
    "\n",
    "\n",
    "        #3. derive proper training inputs for BERT from X_overall_train\n",
    "        #for simpletransformers library, must have 'text' and 'labels' columns\n",
    "        renamed_df=X_overall_train.rename(columns={outcome:'labels'}) # <--------------------------------COME BACK TO THIS PART TOMORROW\n",
    "        X_bert_train=clean_and_featurize(renamed_df,text_name='text')\n",
    "        X_bert_train=X_bert_train[['text','labels']]\n",
    "\n",
    "\n",
    "        #now derive BERT testing dataframe\n",
    "        renamed_test_df=X_overall_test.rename(columns={outcome:'labels'})\n",
    "        X_bert_test=clean_and_featurize(renamed_test_df,text_name='text')\n",
    "        X_bert_test=X_bert_test[['text','labels']]\n",
    "\n",
    "#         print(X_bert_test.shape)\n",
    "#         print(X_bert_test.columns)\n",
    "\n",
    "\n",
    "\n",
    "        #derive weights for training:\n",
    "        num_zeros=X_bert_train[X_bert_train['labels']==0].shape[0]\n",
    "        num_ones=X_bert_train[X_bert_train['labels']==1].shape[0]\n",
    "        #derived weights\n",
    "        weights=[1/num_zeros,1/num_ones]\n",
    "\n",
    "#         print(weights)\n",
    "\n",
    "#         initialize your model\n",
    "        model=ClassificationModel('bert','emilyalsentzer/Bio_ClinicalBERT',\n",
    "                             num_labels=2,use_cuda=True,weight=weights,\n",
    "                             args={'num_train_epochs':1,'output_dir':'mcnemar_outputs/','overwrite_output_dir':True,'max_seq_length':512})\n",
    "\n",
    "        model.train_model(X_bert_train,num_training_epochs=1)\n",
    "        \n",
    "        to_predict=[row['text'] for _,row in X_bert_test.iterrows()]\n",
    "\n",
    "        y_bert_pred,_=model.predict(to_predict) #<----NEED A BERT TESTING \n",
    "\n",
    "#         print(X_bert_train.shape)\n",
    "#         print(y_overall_train.shape)\n",
    "#         print(X_bert_train.columns)\n",
    "#         print(X_lasso_train.columns)\n",
    "\n",
    "\n",
    "        #Now fill in the dataframe from which to derive McNemar cells\n",
    "        df=pd.DataFrame(columns=['lasso_pred','bert_pred','true_label'])\n",
    "        df['true_label']=y_overall_test\n",
    "        df['lasso_pred']=y_lasso_pred\n",
    "        df['bert_pred']=y_bert_pred\n",
    "        print(df)\n",
    "\n",
    "\n",
    "        #now generate McNemar table and print statistic:\n",
    "        mcnemar_table=generate_mcnemar_table(df)\n",
    "        print(mcnemar_table)\n",
    "        print(mcnemar(mcnemar_table))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for trial  1\n",
      "performed first split of the overall data\n",
      "performed featurization and split for Lasso\n",
      "423\n",
      "423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b388b6ce4146427c87b91e034c051821",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1691 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d456bf5b7714fa6a83b2541b37cc4fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ae3610ccbe4c6d91d0eaac7ace13c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mattmill/miniconda3/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:922: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  model.parameters(), args.max_grad_norm\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91f4e670fb74476f81ed31cae565a9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29d8a46c23b245aaa6d74ad6b5122d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      lasso_pred  bert_pred  true_label\n",
      "136            0          0           0\n",
      "2230           1          1           1\n",
      "1080           0          0           0\n",
      "1114           1          1           1\n",
      "444            0          0           0\n",
      "...          ...        ...         ...\n",
      "458            1          1           1\n",
      "1265           1          1           1\n",
      "1723           1          1           1\n",
      "884            0          0           0\n",
      "725            0          0           0\n",
      "\n",
      "[423 rows x 3 columns]\n",
      "[[360   8]\n",
      " [ 41  14]]\n",
      "pvalue      1.9646537765538596e-06\n",
      "statistic   8.0\n",
      "Results for trial  1\n",
      "performed first split of the overall data\n",
      "performed featurization and split for Lasso\n",
      "164\n",
      "164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9862dfe93fdc470d99f2371bf54380ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/654 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "447911551cd14634ae53f8f43006c765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04dc40e8ce4448338871c29420841df7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 1:   0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10964f07c810436991ceef0e7ce65def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/164 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f7a133f721c40f2917658a19d120e54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      lasso_pred  bert_pred  true_label\n",
      "834            0          1           0\n",
      "168            1          1           1\n",
      "2008           0          1           0\n",
      "1426           0          1           0\n",
      "80             0          1           0\n",
      "...          ...        ...         ...\n",
      "1361           0          1           0\n",
      "1941           1          1           0\n",
      "1935           1          1           0\n",
      "888            0          1           1\n",
      "455            0          1           0\n",
      "\n",
      "[164 rows x 3 columns]\n",
      "[[45 76]\n",
      " [24 19]]\n",
      "pvalue      1.810002621302925e-07\n",
      "statistic   24.0\n"
     ]
    }
   ],
   "source": [
    "#for out in ['edema_report','MLS_presence','hem_conv_v2','severe_ecass']:\n",
    "for out in ['edema_report','severe_ecass']:    \n",
    "    loop_mcnemar_calc(input_spreadsheet,out,num_trials=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
