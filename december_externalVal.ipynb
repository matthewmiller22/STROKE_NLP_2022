{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /opt/anaconda3/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import imblearn\n",
    "from sklearn.calibration import calibration_curve,CalibratedClassifierCV\n",
    "import seaborn as sns\n",
    "import pandas as pd, numpy as np\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import re\n",
    "import scipy.stats as st\n",
    "from mord import LogisticIT, LogisticAT\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import confusion_matrix,f1_score,plot_confusion_matrix, classification_report, plot_roc_curve,accuracy_score, f1_score,roc_curve, roc_auc_score,mean_squared_error, mean_absolute_error,r2_score,precision_recall_curve\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.linear_model import LogisticRegression,Ridge,RidgeCV,LassoCV\n",
    "from sklearn.model_selection import GridSearchCV, KFold,RepeatedKFold,train_test_split,RandomizedSearchCV,ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier,RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_featurize(df):\n",
    "    \"\"\"Something is happening here to clean the data....\"\"\"\n",
    "    labeled = df\n",
    "    #labeled = labeled[['Report_ID','Report_Text','MLS_mm']]\n",
    "    \n",
    "    #rename columns to relevant\n",
    "    labeled = labeled.rename(columns={\"Report_ID\": \"Report_Number\"})\n",
    "    \n",
    "    # Keep only those with IMPRESSIONS\n",
    "    labeled = labeled.iloc[[x for x in range(labeled.shape[0]) if 'IMPRESSION:' in labeled.Report_Text.iloc[x]]]\n",
    "    \n",
    "    # replace whitespace with space ***************************\n",
    "    labeled['Report_Text_clean'] = labeled['Report_Text'].apply(lambda text: ' '.join(text.split()))\n",
    "    \n",
    "    #REMOVE HEADER:\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: re.split('-'*78, text, 1)[-1])\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: re.split('HISTORY:', text, 1)[-1])\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: re.split('REPORT ', text, 1)[-1])\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: re.split('REPORT:', text, 1)[-1])\n",
    "    \n",
    "    #REMOVE FOOTER:\n",
    "    # Remove footer parts\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: \n",
    "                                                       re.split('electronically signed by:', text, flags=re.IGNORECASE)[0])\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: \n",
    "                                                       ''.join(re.split('i, the teaching physician, have reviewed the images and agree with the report as written', text, flags=re.IGNORECASE)))\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: \n",
    "                                                       re.split('radiologists: signatures:', text, flags=re.IGNORECASE)[0])\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: \n",
    "                                                       re.split('providers: signatures:', text, flags=re.IGNORECASE)[0])\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: \n",
    "                                                       re.split('findings were discussed on', text, flags=re.IGNORECASE)[0])\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: \n",
    "                                                       re.split('this report was electronically signed by', text, flags=re.IGNORECASE)[0])\n",
    "\n",
    "    # Remove reference texts =====\n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: ''.join([x for i,x in enumerate(text.split('='*34)) if i != 1]))\n",
    "    \n",
    "    labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: text.lower())\n",
    "    \n",
    "    \n",
    "    # Replace ngrams in Report_Text & IMPRESSION with their units\n",
    "    for group in wordgroups:\n",
    "        labeled['Report_Text_clean'] = labeled.Report_Text_clean.apply(lambda text: text.replace(group, ''.join(group.split())))\n",
    "\n",
    "    #labeled = labeled.drop_duplicates(subset=['Report_Number'])\n",
    "    labeled = labeled.reset_index(drop=True)\n",
    "    \n",
    "    return labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_confidence_interval(data, confidence=0.95):\n",
    "    a = 1.0 * np.array(data)\n",
    "    n = len(a)\n",
    "    m, se = np.mean(a), st.sem(a)\n",
    "    h = se * st.t.ppf((1 + confidence) / 2., n-1)\n",
    "    return m, m-h, m+h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_externalValidation_BOW_matrix(external_val_df,fit_df,vectorizer='count',doc_col_name=\"Report_Text_clean\"):\n",
    "    \"\"\"Trains a vectorizer on the FIT DATASET and then applies it to an EXTERNAL VALIDATION set. The\n",
    "    result is once more a matrix in which entries are token counts per document. Only returns this matrix\n",
    "    as a dataframe for the EXTERNAL VALIDATION set; at present, the \n",
    "    \n",
    "    :params\n",
    "    1) external_val_df: the preprocessed dataframe of stroke reports from your external validation dataset\n",
    "    2) fit_df: the preprocessed data for the FITTING dataset (MGB large acute strokes)\n",
    "    3) vectorizer: determines count or TF-IDF vectorization\"\"\"\n",
    "    #initialize vectorizer\n",
    "    if vectorizer=='count':\n",
    "        vectorizer = CountVectorizer(min_df=0.01) #ignore if term appears in <cutoff % of documents\n",
    "    else:\n",
    "        vectorizer=TfidfVectorizer(min_df=0.01)\n",
    "        \n",
    "        \n",
    "    #1) fit the vectorizer on the training data\n",
    "    #Obtain counts using the column name corresponding to the radiology reports\n",
    "    vectorizer.fit(external_val_df[doc_col_name])\n",
    "    \n",
    "    #\n",
    "    fit_inTermsof_extVal=vectorizer.transform(fit_df[doc_col_name]).todense() #now transform train dataset in terms of test features\n",
    "    extVal_inTermsof_extVal=vectorizer.transform(external_val_df[doc_col_name]).todense()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #3) Obtain feature names for dataframe columns\n",
    "    feature_names=vectorizer.get_feature_names()\n",
    "    \n",
    "    #this is the train set outcomes in terms of the test set features\n",
    "    X_fit=pd.DataFrame(fit_inTermsof_extVal,columns=feature_names)\n",
    "    X_extVal=pd.DataFrame(extVal_inTermsof_extVal,columns=feature_names)\n",
    "    \n",
    "    return X_fit,X_extVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extVal_classifier_complete(n_trials,cleaned_fit_df,cleaned_extVal_df,fit_BOW_matrix,extVal_BOW_matrix,doc_col_name,classifier,print_complete=True):\n",
    "    \"\"\"A comprehensive function that takes in all data and a classifier of choice then computes \n",
    "    epidemiologic statistics for that classifier as well as returns numpy arrays corresponding to \n",
    "    ROC, PR, and calibration curves. Hyperparameter tuning is included within classifier definitions, which\n",
    "    occur within the body of the function as below\"\"\"\n",
    "    \n",
    "    num_trials = n_trials\n",
    "    score = 'roc_auc' #metric by which to evaluate\n",
    "    \n",
    "    ###########GRAPHS###########################\n",
    "    \n",
    "    #ROC\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    \n",
    "    #PR\n",
    "    \n",
    "    precisions_4graph = [] #this vector is distinct from the epdemiologic measures of \"precision\"\n",
    "    recalls = []\n",
    "    \n",
    "    #Calibration curves\n",
    "    mpvs = []\n",
    "    fops = []\n",
    "    \n",
    "    \n",
    "    #########STATS##############################\n",
    "    #statistics\n",
    "    accuracies=[]\n",
    "    aucs=[]\n",
    "    auprs=[]\n",
    "    specificities=[]\n",
    "    sensitivities=[]\n",
    "    precisions=[]\n",
    "    f1s=[]\n",
    "    \n",
    "    \n",
    "    #STATS\n",
    "    accuracy=0\n",
    "    aupr=0\n",
    "    auc = 0 #will keep track of average AUC at the end of this all\n",
    "    specificity=0\n",
    "    sensitivity=0\n",
    "    precision=0\n",
    "    f1=0\n",
    "    \n",
    "    sm = SMOTE(random_state=42)\n",
    "    #oversample = imblearn.RandomOverSampler(sampling_strategy='minority')\n",
    "    \n",
    "    for trial in range(1,num_trials+1):\n",
    "        print('Results for trial ',trial)\n",
    "        X_train,_,y_train,_ = train_test_split(fit_BOW_matrix,cleaned_fit_df[str(doc_col_name)],test_size=0.20)\n",
    "        \n",
    "        X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "        \n",
    "        if classifier=='logreg':\n",
    "        \n",
    "            tuned_parameters =[{'tol': np.arange(0.0001, 0.1 + 0.01, 0.01)}]\n",
    "\n",
    "            #initialize Logistic Regression with GridSearch over 'tolerance' parameter\n",
    "            outcome_presence = GridSearchCV(LogisticRegression(penalty = 'l1',max_iter=1000, random_state=100, solver='liblinear'), \n",
    "                                        tuned_parameters, cv = 10, scoring = 'roc_auc')\n",
    "            \n",
    "                    #Calibrate the classifier with Platt scaling\n",
    "            outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "            \n",
    "            \n",
    "        elif classifier=='rf':\n",
    "            \n",
    "            maxTrees = 200\n",
    "            maxDepth = 10\n",
    "            minBucket = 10\n",
    "            \n",
    "            tuned_parameters = [{'n_estimators': range(10,maxTrees+10,100),'max_depth':range(3,maxDepth+1), 'min_samples_leaf':range(1,minBucket+1), 'bootstrap' : [True, False]}]\n",
    "        \n",
    "            outcome_presence = RandomizedSearchCV(RandomForestClassifier(criterion=\"gini\", random_state=100),\n",
    "                               tuned_parameters, cv=10, scoring = score) \n",
    "            \n",
    "            outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "            \n",
    "        elif classifier=='nn':\n",
    "            mlp_parameters = [{'hidden_layer_sizes': [50,100,200]}]\n",
    "        \n",
    "        #initialize Logistic Regression with GridSearch over 'tolerance' parameter\n",
    "            outcome_presence = GridSearchCV(MLPClassifier(learning_rate_init=0.003), \n",
    "                                        mlp_parameters, cv = 5, scoring = 'roc_auc')\n",
    "            \n",
    "        elif classifier=='CART':\n",
    "            maxDepth = 10\n",
    "            minBucket = 10\n",
    "            tuned_parameters = [{'max_depth':range(3,maxDepth+1), 'min_samples_leaf':range(1,minBucket+1)}]\n",
    "            outcome_presence = GridSearchCV(tree.DecisionTreeClassifier(criterion=\"gini\", random_state=100, splitter = 'best'),\n",
    "                               tuned_parameters, cv=10, scoring = score)\n",
    "        \n",
    "            #Calibrate the classifier with Platt scaling\n",
    "            outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "            \n",
    "        elif classifier=='svm':\n",
    "            tuned_parameters = {'C': [0.1,], \n",
    "                          'gamma': [0.1, 0.01, 0.001]} \n",
    "            outcome_presence=GridSearchCV(SVC(),tuned_parameters,cv=5,scoring=score)\n",
    "            outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "            \n",
    "        elif classifier=='knn':\n",
    "            max_neighbors=7\n",
    "            tuned_parameters=[{'n_neighbors':range(2,max_neighbors)}]\n",
    "            outcome_presence = GridSearchCV(KNeighborsClassifier(),\n",
    "                   tuned_parameters, cv=10, scoring = score)\n",
    "            outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "            \n",
    "        \n",
    "        #SMOTE resampling\n",
    "        #outcome_presence.fit(X_train,y_train)\n",
    "        \n",
    "        #external testing data \n",
    "        X_test,y_test=extVal_BOW_matrix,cleaned_extVal_df[str(doc_col_name)]\n",
    "        \n",
    "        outcome_presence.fit(X_res, y_res)\n",
    "        \n",
    "        #categorical predictions\n",
    "        y_pred = outcome_presence.predict(X_test)\n",
    "        #raw probabilities\n",
    "        y_proba = outcome_presence.predict_proba(X_test)[:,1]\n",
    "        \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "        print(type(y_pred))\n",
    "        \n",
    "        if print_complete:\n",
    "            print(f'TP: {tp} \\n')\n",
    "            print(f'TN: {tn} \\n')\n",
    "            print(f'FP: {fp} \\n')\n",
    "            print(f'FN: {fn} \\n')\n",
    "            print(3*'\\n')\n",
    "\n",
    "                \n",
    "        #for optional plotting:\n",
    "        fpr,tpr,_ = roc_curve(y_test,y_proba) #ROC\n",
    "        pr_4graph,recall,_ = precision_recall_curve(y_test,y_proba) #PR\n",
    "        fop, mpv = calibration_curve(y_test, y_proba, n_bins=10) #calibration\n",
    "\n",
    "        \n",
    "        #ROC Curve\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "        \n",
    "        #PR Curve\n",
    "        precisions_4graph.append(pr_4graph)\n",
    "        recalls.append(recall)\n",
    "        \n",
    "        #Calibration Curve\n",
    "        mpvs.append(mpv)\n",
    "        fops.append(fop)\n",
    "        \n",
    "        \n",
    "        \n",
    "        #print(roc_auc_score(y_test,y_proba))\n",
    "        \n",
    "        #for specific trial\n",
    "        trial_auc=roc_auc_score(y_test, y_proba)\n",
    "        trial_aupr=average_precision_score(y_test,y_proba)\n",
    "        trial_sensitivity=tp/(tp+fn)\n",
    "        trial_specificity=tn/(tn+fp)\n",
    "        trial_precision=tp/(tp+fp)\n",
    "        trial_f1=f1_score(y_test,y_pred)\n",
    "        trial_accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "        \n",
    "        #for averaging over all trials\n",
    "        auc += trial_auc #update running sum of AUC scores\n",
    "        sensitivity+=trial_sensitivity\n",
    "        aupr+=trial_aupr\n",
    "        specificity+=trial_specificity\n",
    "        precision+=trial_precision\n",
    "        f1+=trial_f1\n",
    "        accuracy+=trial_accuracy\n",
    "        \n",
    "        #for standard dev over all trials \n",
    "        aucs.append(trial_auc)\n",
    "        sensitivities.append(trial_sensitivity)\n",
    "        auprs.append(trial_aupr)\n",
    "        specificities.append(trial_specificity)\n",
    "        precisions.append(trial_precision)\n",
    "        f1s.append(trial_f1)\n",
    "        accuracies.append(trial_accuracy)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #average values \n",
    "    average_AUC = auc/n_trials\n",
    "    average_AUPR=aupr/n_trials\n",
    "    average_sensitivity=sensitivity/n_trials\n",
    "    average_specificity=specificity/n_trials\n",
    "    average_precision=precision/n_trials\n",
    "    average_f1=f1/n_trials\n",
    "    average_accuracy=accuracy/n_trials\n",
    "    \n",
    "    \n",
    "    #standard deviations\n",
    "#     std_auc=np.round(np.std(aucs),3)\n",
    "#     std_sens=np.round(np.std(sensitivities),3)\n",
    "#     std_spec=np.round(np.std(specificities),3)\n",
    "#     std_prec=np.round(np.std(precisions),3)\n",
    "#     std_f1=np.round(np.std(f1s),3)\n",
    "#     std_acc=np.round(np.std(accuracies),3)\n",
    "    \n",
    "    #95th %ile Confidence Intervals:\n",
    "    ci95_auc=mean_confidence_interval(aucs)\n",
    "    ci95_aupr=mean_confidence_interval(auprs)\n",
    "    ci95_sens=mean_confidence_interval(sensitivities)\n",
    "    ci95_spec=mean_confidence_interval(specificities)\n",
    "    ci95_prec=mean_confidence_interval(precisions)\n",
    "    ci95_acc=mean_confidence_interval(accuracies)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    print(f'{classifier} for {doc_col_name}.\\n ======================================\\n')\n",
    "    \n",
    "    print(f'AUC [{ci95_auc}]. AUPR [{ci95_aupr}]. Sensitivity [{ci95_sens}]. Specificity[{ci95_spec}]\\n')\n",
    "    print(f'Precision [{ci95_prec}]. Accuracy [{ci95_acc}]')\n",
    "\n",
    "    ###RETURN THE ARRAYS FOR THE GRAPHS\n",
    "    return fprs,tprs,precisions_4graph,recalls,mpvs,fops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extVal_classifier_complete(n_trials,cleaned_fit_df,cleaned_extVal_df,fit_BOW_matrix,extVal_BOW_matrix,doc_col_name,classifier,print_complete=True):\n",
    "#     \"\"\"A comprehensive function that takes in all data and a classifier of choice then computes \n",
    "#     epidemiologic statistics for that classifier as well as returns numpy arrays corresponding to \n",
    "#     ROC, PR, and calibration curves. Hyperparameter tuning is included within classifier definitions, which\n",
    "#     occur within the body of the function as below\"\"\"\n",
    "    \n",
    "#     num_trials = n_trials\n",
    "#     score = 'roc_auc' #metric by which to evaluate\n",
    "    \n",
    "#     ###########GRAPHS###########################\n",
    "    \n",
    "#     #ROC\n",
    "#     fprs = []\n",
    "#     tprs = []\n",
    "    \n",
    "#     #PR\n",
    "    \n",
    "#     precisions_4graph = [] #this vector is distinct from the epdemiologic measures of \"precision\"\n",
    "#     recalls = []\n",
    "    \n",
    "#     #Calibration curves\n",
    "#     mpvs = []\n",
    "#     fops = []\n",
    "    \n",
    "    \n",
    "#     #########STATS##############################\n",
    "#     #statistics\n",
    "#     accuracies=[]\n",
    "#     aucs=[]\n",
    "#     specificities=[]\n",
    "#     sensitivities=[]\n",
    "#     precisions=[]\n",
    "#     f1s=[]\n",
    "    \n",
    "    \n",
    "#     #STATS\n",
    "#     accuracy=0\n",
    "#     auc = 0 #will keep track of average AUC at the end of this all\n",
    "#     specificity=0\n",
    "#     sensitivity=0\n",
    "#     precision=0\n",
    "#     f1=0\n",
    "    \n",
    "#     sm = SMOTE(random_state=42)\n",
    "    \n",
    "#     for trial in range(1,num_trials+1):\n",
    "#         print('Results for trial ',trial)\n",
    "#         X_train,_,y_train,_ = train_test_split(fit_BOW_matrix,cleaned_fit_df[str(doc_col_name)],test_size=0.20)\n",
    "        \n",
    "#         X_res, y_res = sm.fit_resample(X_train, y_train)\n",
    "        \n",
    "#         if classifier=='logreg':\n",
    "        \n",
    "#             tuned_parameters =[{'tol': np.arange(0.0001, 0.1 + 0.01, 0.01)}]\n",
    "\n",
    "#             #initialize Logistic Regression with GridSearch over 'tolerance' parameter\n",
    "#             outcome_presence = GridSearchCV(LogisticRegression(penalty = 'l1',max_iter=1000, random_state=100, solver='liblinear'), \n",
    "#                                         tuned_parameters, cv = 10, scoring = 'roc_auc')\n",
    "            \n",
    "#                     #Calibrate the classifier with Platt scaling\n",
    "#             outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "            \n",
    "            \n",
    "#         elif classifier=='rf':\n",
    "            \n",
    "#             maxTrees = 200\n",
    "#             maxDepth = 10\n",
    "#             minBucket = 10\n",
    "            \n",
    "#             tuned_parameters = [{'n_estimators': range(10,maxTrees+10,100),'max_depth':range(3,maxDepth+1), 'min_samples_leaf':range(1,minBucket+1), 'bootstrap' : [True, False]}]\n",
    "        \n",
    "#             outcome_presence = RandomizedSearchCV(RandomForestClassifier(criterion=\"gini\", random_state=100),\n",
    "#                                tuned_parameters, cv=10, scoring = score) \n",
    "            \n",
    "#             outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "            \n",
    "#         elif classifier=='nn':\n",
    "#             mlp_parameters = [{'hidden_layer_sizes': [50,100,200]}]\n",
    "        \n",
    "#         #initialize Logistic Regression with GridSearch over 'tolerance' parameter\n",
    "#             outcome_presence = GridSearchCV(MLPClassifier(learning_rate_init=0.003), \n",
    "#                                         mlp_parameters, cv = 5, scoring = 'roc_auc')\n",
    "            \n",
    "#         elif classifier=='CART':\n",
    "#             maxDepth = 10\n",
    "#             minBucket = 10\n",
    "#             tuned_parameters = [{'max_depth':range(3,maxDepth+1), 'min_samples_leaf':range(1,minBucket+1)}]\n",
    "#             outcome_presence = GridSearchCV(tree.DecisionTreeClassifier(criterion=\"gini\", random_state=100, splitter = 'best'),\n",
    "#                                tuned_parameters, cv=10, scoring = score)\n",
    "        \n",
    "#             #Calibrate the classifier with Platt scaling\n",
    "#             outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "            \n",
    "#         elif classifier=='svm':\n",
    "#             tuned_parameters = {'C': [0.1,], \n",
    "#                           'gamma': [0.1, 0.01, 0.001]} \n",
    "#             outcome_presence=GridSearchCV(SVC(),tuned_parameters,cv=5,scoring=score)\n",
    "#             outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "            \n",
    "#         elif classifier=='knn':\n",
    "#             max_neighbors=7\n",
    "#             tuned_parameters=[{'n_neighbors':range(2,max_neighbors)}]\n",
    "#             outcome_presence = GridSearchCV(KNeighborsClassifier(),\n",
    "#                    tuned_parameters, cv=10, scoring = score)\n",
    "#             outcome_presence=CalibratedClassifierCV(outcome_presence)\n",
    "            \n",
    "        \n",
    "#         #SMOTE resampling\n",
    "#         outcome_presence.fit(X_res,y_res)\n",
    "        \n",
    "#         #external testing data \n",
    "#         X_test,y_test=extVal_BOW_matrix,cleaned_extVal_df[str(doc_col_name)]\n",
    "        \n",
    "#         #outcome_presence.fit(X_train, y_train)\n",
    "        \n",
    "#         #categorical predictions\n",
    "#         y_pred = outcome_presence.predict(X_test)\n",
    "#         #raw probabilities\n",
    "#         y_proba = outcome_presence.predict_proba(X_test)[:,1]\n",
    "        \n",
    "#         tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        \n",
    "#         print(type(y_pred))\n",
    "        \n",
    "#         if print_complete:\n",
    "#             print(f'TP: {tp} \\n')\n",
    "#             print(f'TN: {tn} \\n')\n",
    "#             print(f'FP: {fp} \\n')\n",
    "#             print(f'FN: {fn} \\n')\n",
    "#             print(3*'\\n')\n",
    "\n",
    "                \n",
    "#         #for optional plotting:\n",
    "#         fpr,tpr,_ = roc_curve(y_test,y_proba) #ROC\n",
    "#         pr_4graph,recall,_ = precision_recall_curve(y_test,y_proba) #PR\n",
    "#         fop, mpv = calibration_curve(y_test, y_proba, n_bins=10) #calibration\n",
    "\n",
    "        \n",
    "#         #ROC Curve\n",
    "#         fprs.append(fpr)\n",
    "#         tprs.append(tpr)\n",
    "        \n",
    "#         #PR Curve\n",
    "#         precisions_4graph.append(pr_4graph)\n",
    "#         recalls.append(recall)\n",
    "        \n",
    "#         #Calibration Curve\n",
    "#         mpvs.append(mpv)\n",
    "#         fops.append(fop)\n",
    "        \n",
    "        \n",
    "        \n",
    "#         #print(roc_auc_score(y_test,y_proba))\n",
    "        \n",
    "#         #for specific trial\n",
    "#         trial_auc=roc_auc_score(y_test, y_proba)\n",
    "#         trial_sensitivity=tp/(tp+fn)\n",
    "#         trial_specificity=tn/(tn+fp)\n",
    "#         trial_precision=tp/(tp+fp)\n",
    "#         trial_f1=f1_score(y_test,y_pred)\n",
    "#         trial_accuracy=(tp+tn)/(tp+tn+fp+fn)\n",
    "        \n",
    "#         #for averaging over all trials\n",
    "#         auc += trial_auc #update running sum of AUC scores\n",
    "#         sensitivity+=trial_sensitivity\n",
    "#         specificity+=trial_specificity\n",
    "#         precision+=trial_precision\n",
    "#         f1+=trial_f1\n",
    "#         accuracy+=trial_accuracy\n",
    "        \n",
    "#         #for standard dev over all trials \n",
    "#         aucs.append(trial_auc)\n",
    "#         sensitivities.append(trial_sensitivity)\n",
    "#         specificities.append(trial_specificity)\n",
    "#         precisions.append(trial_precision)\n",
    "#         f1s.append(trial_f1)\n",
    "#         accuracies.append(trial_accuracy)\n",
    "        \n",
    "        \n",
    "        \n",
    "#     #average values \n",
    "#     average_AUC = auc/n_trials\n",
    "#     average_sensitivity=sensitivity/n_trials\n",
    "#     average_specificity=specificity/n_trials\n",
    "#     average_precision=precision/n_trials\n",
    "#     average_f1=f1/n_trials\n",
    "#     average_accuracy=accuracy/n_trials\n",
    "    \n",
    "    \n",
    "#     #standard deviations\n",
    "# #     std_auc=np.round(np.std(aucs),3)\n",
    "# #     std_sens=np.round(np.std(sensitivities),3)\n",
    "# #     std_spec=np.round(np.std(specificities),3)\n",
    "# #     std_prec=np.round(np.std(precisions),3)\n",
    "# #     std_f1=np.round(np.std(f1s),3)\n",
    "# #     std_acc=np.round(np.std(accuracies),3)\n",
    "    \n",
    "#     #95th %ile Confidence Intervals:\n",
    "#     ci95_auc=st.t.interval(alpha=0.95, df=len(aucs)-1, loc=np.mean(aucs), scale=st.sem(aucs))\n",
    "#     ci95_sens=st.t.interval(alpha=0.95, df=len(sensitivities)-1, loc=np.mean(sensitivities), scale=st.sem(sensitivities))\n",
    "#     ci95_spec=st.t.interval(alpha=0.95, df=len(specificities)-1, loc=np.mean(specificities), scale=st.sem(specificities))\n",
    "#     ci95_prec=st.t.interval(alpha=0.95, df=len(precisions)-1, loc=np.mean(precisions), scale=st.sem(precisions))\n",
    "#     ci95_f1=st.t.interval(alpha=0.95, df=len(f1s)-1, loc=np.mean(f1s), scale=st.sem(f1s))\n",
    "#     ci95_acc=st.t.interval(alpha=0.95, df=len(accuracies)-1, loc=np.mean(accuracies), scale=st.sem(accuracies))\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print(f'{classifier} for {doc_col_name}.\\n ======================================\\n')\n",
    "#     print(f'AUC {average_AUC} [{ci95_auc}]. Sensitivity {average_sensitivity} [{ci95_sens}]. Specificity {average_specificity} [{ci95_spec}]\\n')\n",
    "#     print(f'Precision {average_precision} [{ci95_prec}]. F1 {average_f1} [{ci95_f1}]. Accuracy {average_accuracy} [{ci95_acc}]')\n",
    "    \n",
    "\n",
    "#     ###RETURN THE ARRAYS FOR THE GRAPHS\n",
    "#     return fprs,tprs,precisions_4graph,recalls,mpvs,fops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data 0: Fit DF (BWH Large Stroke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('6.1.21 deidComplete REDCap Reports Non-duplicate Version 3.csv')\n",
    "df.rename(columns={'report_text': 'Report_Text', 'mls_mm_v2': 'MLS_mm','record_id':'Report_ID','ecass_v2':'ECASS','edema_severity_report':'edema_severity'}, inplace=True)\n",
    "\n",
    "df=df.dropna(subset=['ivh_v2'])\n",
    "df['ivh_present']=df['ivh_v2'].apply(lambda x: 0 if x==1.0 else 1)\n",
    "df['ECASS']=df['ECASS'].fillna(0)\n",
    "df['severe_ecass']=df['ECASS'].apply(lambda x: 0 if x<3.0 else 1)\n",
    "df['MLS_mm']=df['MLS_mm'].fillna(0)\n",
    "df['MLS_presence']=df['MLS_mm'].apply(lambda x: 0 if x==0 else 1)\n",
    "#df_edema = df[np.logical_or(df['MLS_presence']==0,df['MLS_presence']==1)]\n",
    "df['edema_report']=df['edema_report'].fillna(0)\n",
    "df['edema_report']=df['edema_report'].apply(lambda x: int(x))\n",
    "\n",
    "wordgroups = list(pd.read_excel('wordgroups.xlsx')['Word Groupings'])#now clean and featurize\n",
    "fit_df_overall = clean_and_featurize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data 1: BMC Below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=pd.read_csv('OCTOBER8_377_BMC_ACUTE_STROKE_CONFIRMED.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_extVal_df = pd.read_csv('OCTOBER8_377_BMC_ACUTE_STROKE_CONFIRMED.csv')\n",
    "\n",
    "\n",
    "clean_extVal_df.rename(columns={'ecass':'ECASS','ivh':'ivh_present','edema':'edema_report','hemcon':'hem_conv_v2','midline_shift_mm':'MLS_mm'}, inplace=True)\n",
    "\n",
    "# clean_extVal_df=clean_extVal_df.dropna(subset=['ivh_v2'])\n",
    "# clean_extVal_df['ivh_present']=clean_extVal_df['ivh_v2'].apply(lambda x: 0 if x==1.0 else 1)\n",
    "clean_extVal_df['ECASS']=clean_extVal_df['ECASS'].fillna(0)\n",
    "clean_extVal_df['severe_ecass']=clean_extVal_df['ECASS'].apply(lambda x: 0 if x<3.0 else 1)\n",
    "#clean_extVal_df['MLS_mm']=clean_extVal_df['MLS_mm'].fillna(0)\n",
    "\n",
    "clean_extVal_df['MLS_presence']=clean_extVal_df['MLS_mm'].apply(lambda x: 0 if np.logical_or(x==0,x==999) else 1)\n",
    "# #clean_extVal_df_edema = clean_extVal_df[np.logical_or(clean_extVal_df['MLS_presence']==0,clean_extVal_df['MLS_presence']==1)]\n",
    "clean_extVal_df['edema_report']=clean_extVal_df['edema_report'].fillna(0)\n",
    "clean_extVal_df['edema_report']=clean_extVal_df['edema_report'].apply(lambda x: int(x))\n",
    "\n",
    "wordgroups = list(pd.read_excel('wordgroups.xlsx')['Word Groupings'])#now clean and featurize\n",
    "clean_extVal_df = clean_and_featurize(clean_extVal_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_extVal_df=pd.read_csv('bwh_strokes_not_just_large_v2.csv')\n",
    "# clean_extVal_df=clean_extVal_df.rename(columns={'Report_Text':'Report_Text_clean','mls_mm_v2':'MLS_mm','ecass_v2':'ECASS'})\n",
    "# clean_extVal_df=clean_extVal_df.dropna(subset=['ivh_v2'])\n",
    "# clean_extVal_df['ivh_present']=clean_extVal_df['ivh_v2'].apply(lambda x: 0 if x==1.0 else 1)\n",
    "# clean_extVal_df['ECASS']=clean_extVal_df['ECASS'].fillna(0)\n",
    "# clean_extVal_df['severe_ecass']=clean_extVal_df['ECASS'].apply(lambda x: 0 if x<3.0 else 1)\n",
    "# clean_extVal_df['MLS_mm']=clean_extVal_df['MLS_mm'].fillna(0)\n",
    "# clean_extVal_df['MLS_presence']=clean_extVal_df['MLS_mm'].apply(lambda x: 0 if x==0 else 1)\n",
    "# #clean_extVal_df_edema = clean_extVal_df[np.logical_or(clean_extVal_df['MLS_presence']==0,clean_extVal_df['MLS_presence']==1)]\n",
    "# clean_extVal_df['edema_report']=clean_extVal_df['edema_report'].fillna(0)\n",
    "# clean_extVal_df['edema_report']=clean_extVal_df['edema_report'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare MGB Non-Large Data Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgb_nonlarge1=pd.read_csv('OCTOBER15_BWH_NONLARGE_BATCH1.csv')\n",
    "mgb_nonlarge2=pd.read_csv('OCTOBER15_BWH_NONLARGE_BATCH2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Exam Number:  E1082208                        ...\n",
       "1      Exam Number:  16915447                        ...\n",
       "2      Exam Number:  17732643                        ...\n",
       "3      Exam Number:  A13814506                       ...\n",
       "4      Exam Number:  17173917                        ...\n",
       "                             ...                        \n",
       "332    Exam Number:  18223378                        ...\n",
       "333    Exam Number:  E1300108                        ...\n",
       "334    Exam Number:  17081936                        ...\n",
       "335    Exam Number:  18202901                        ...\n",
       "336    Exam Number:  16821557                        ...\n",
       "Name: Unprocessed_Report_Text, Length: 337, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mgb_nonlarge2['Unprocessed_Report_Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "mgb_nonlarge1=pd.read_csv('OCTOBER15_BWH_NONLARGE_BATCH1.csv')\n",
    "mgb_nonlarge2=pd.read_csv('OCTOBER15_BWH_NONLARGE_BATCH2.csv')\n",
    "mgb_nonlarge2=mgb_nonlarge2[['Report Number','Unprocessed_Report_Text','EMPI','Report_Description','Age','Gender','edema_report','mls_mm_v2','hem_conv_v2','ecass_v2','ivh_v2']]\n",
    "mgb_nonlarge1=mgb_nonlarge1[['Report_Number','Unprocessed_Report_Text','EMPI','Report_Description','Age','Gender','edema_report','mls_mm_v2','hem_conv_v2','ecass_v2','ivh_v2']]\n",
    "clean_extVal_df=pd.concat([mgb_nonlarge1,mgb_nonlarge2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_extVal_df.rename(columns={'Unprocessed_Report_Text':'Report_Text','ecass_v2':'ECASS','mls_mm_v2':'MLS_mm'}, inplace=True)\n",
    "\n",
    "clean_extVal_df=clean_extVal_df.dropna(subset=['ivh_v2'])\n",
    "clean_extVal_df['ivh_present']=clean_extVal_df['ivh_v2'].apply(lambda x: 0 if x==1.0 else 1)\n",
    "clean_extVal_df['ECASS']=clean_extVal_df['ECASS'].fillna(0)\n",
    "clean_extVal_df['severe_ecass']=clean_extVal_df['ECASS'].apply(lambda x: 0 if x<3.0 else 1)\n",
    "\n",
    "clean_extVal_df['MLS_mm']=clean_extVal_df['MLS_mm'].fillna(0)\n",
    "\n",
    "clean_extVal_df['MLS_presence']=clean_extVal_df['MLS_mm'].apply(lambda x: 0 if np.logical_or(x==0,x==999) else 1)\n",
    "# #clean_extVal_df_edema = clean_extVal_df[np.logical_or(clean_extVal_df['MLS_presence']==0,clean_extVal_df['MLS_presence']==1)]\n",
    "clean_extVal_df['edema_report']=clean_extVal_df['edema_report'].fillna(0)\n",
    "clean_extVal_df['edema_report']=clean_extVal_df['edema_report'].apply(lambda x: int(x))\n",
    "\n",
    "wordgroups = list(pd.read_excel('wordgroups.xlsx')['Word Groupings'])#now clean and featurize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_extVal_df = clean_and_featurize(clean_extVal_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### #RunIt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now testing logreg with ivh_present and count vectorizer\n",
      "Results for trial  1\n",
      "<class 'numpy.ndarray'>\n",
      "TP: 7 \n",
      "\n",
      "TN: 490 \n",
      "\n",
      "FP: 10 \n",
      "\n",
      "FN: 11 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Results for trial  2\n",
      "<class 'numpy.ndarray'>\n",
      "TP: 9 \n",
      "\n",
      "TN: 490 \n",
      "\n",
      "FP: 10 \n",
      "\n",
      "FN: 9 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Results for trial  3\n",
      "<class 'numpy.ndarray'>\n",
      "TP: 11 \n",
      "\n",
      "TN: 492 \n",
      "\n",
      "FP: 8 \n",
      "\n",
      "FN: 7 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "logreg for ivh_present.\n",
      " ======================================\n",
      "\n",
      "AUC [(0.9045925925925927, 0.8746397390138012, 0.9345454461713842)]. AUPR [(0.5256007918949691, 0.39309732634283756, 0.6581042574471007)]. Sensitivity [(0.5, 0.22398469868402737, 0.7760153013159726)]. Specificity[(0.9813333333333333, 0.9755964630267849, 0.9870702036398816)]\n",
      "\n",
      "Precision [(0.48813209494324045, 0.27816636416137014, 0.6980978257251107)]. Accuracy [(0.9646074646074645, 0.9499565648632967, 0.9792583643516323)]\n",
      "Now testing logreg with ivh_present and tfidf vectorizer\n",
      "Results for trial  1\n",
      "<class 'numpy.ndarray'>\n",
      "TP: 17 \n",
      "\n",
      "TN: 488 \n",
      "\n",
      "FP: 12 \n",
      "\n",
      "FN: 1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Results for trial  2\n",
      "<class 'numpy.ndarray'>\n",
      "TP: 17 \n",
      "\n",
      "TN: 498 \n",
      "\n",
      "FP: 2 \n",
      "\n",
      "FN: 1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Results for trial  3\n",
      "<class 'numpy.ndarray'>\n",
      "TP: 17 \n",
      "\n",
      "TN: 494 \n",
      "\n",
      "FP: 6 \n",
      "\n",
      "FN: 1 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "logreg for ivh_present.\n",
      " ======================================\n",
      "\n",
      "AUC [(0.9952592592592593, 0.9907210662943372, 0.9997974522241814)]. AUPR [(0.9404391999370095, 0.8927698839756592, 0.9881085158983599)]. Sensitivity [(0.9444444444444443, 0.944444444444444, 0.9444444444444446)]. Specificity[(0.9866666666666667, 0.9616602287482231, 1.0116731045851102)]\n",
      "\n",
      "Precision [(0.7400247244798653, 0.3568044585868812, 1.1232449903728494)]. Accuracy [(0.9851994851994852, 0.9610619968612193, 1.009336973537751)]\n"
     ]
    }
   ],
   "source": [
    "for classifier in ['logreg']:#,'knn','rf','nn']:\n",
    "#for classifier in ['logreg']:\n",
    "    #for outcome in ['ivh_present','hem_conv_v2','severe_ecass','edema_report','MLS_presence','hem_conv_v2',]:\n",
    "\n",
    "    #for outcome in ['severe_ecass']:\n",
    "    for outcome in ['ivh_present']: #hem_conv_v2','severe_ecass','ivh_present']:\n",
    "    #for outcome in ['edema_report']:\n",
    "    \n",
    "        for vectorizer in ['count','tfidf']:\n",
    "\n",
    "            print(f'Now testing {classifier} with {outcome} and {vectorizer} vectorizer')\n",
    "\n",
    "\n",
    "            fit_df=fit_df_overall[np.logical_or(fit_df_overall[outcome]==0,fit_df_overall[outcome]==1)]\n",
    "            clean_extVal_df=clean_extVal_df[np.logical_or(clean_extVal_df[outcome]==0,clean_extVal_df[outcome]==1)]\n",
    "            X_fit,X_extval=create_externalValidation_BOW_matrix(clean_extVal_df,fit_df,vectorizer=vectorizer)\n",
    "\n",
    "            \n",
    "            \n",
    "            #Run Unique Classifier\n",
    "            fprs,tprs,precisions,recalls,mpvs,fops=extVal_classifier_complete(3,fit_df,clean_extVal_df,X_fit,X_extval,outcome,classifier,print_complete=True)\n",
    "            \n",
    "            \n",
    "            np.save(f'numpys/MGB_nonLarge_{classifier}_{vectorizer}_{outcome}_fprs.npy',fprs)\n",
    "            np.save(f'numpys/MGB_nonLarge_{classifier}_{vectorizer}_{outcome}_tprs.npy',tprs)\n",
    "            np.save(f'numpys/MGB_nonLarge_{classifier}_{vectorizer}_{outcome}_precisions.npy',precisions)\n",
    "            np.save(f'numpys/MGB_nonLarge_{classifier}_{vectorizer}_{outcome}_recalls.npy',recalls)\n",
    "            np.save(f'numpys/MGB_nonLarge_{classifier}_{vectorizer}_{outcome}_mpvs.npy',mpvs)\n",
    "            np.save(f'numpys/MGB_nonLarge_{classifier}_{vectorizer}_{outcome}_fops.npy',fops)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('OCTOBER1_BMC_SPREADSHEET_WITH_DEMOGRAPHICS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last=df.iloc[-350:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_last.to_csv('OCTOBER3_BMC_SPREADSHEET_LAST_ROWS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
